{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About \n",
    "This notebook is a new way to do the analysis.  Starting from the data and ending at the phi-dependent asymmetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "AXES = ['x', 'z', 'pt', 'q2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_RANGE = [0.25, 0.75]\n",
    "N_BINS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the data from a compressed csv file.  Dropping the events which are not used in the nominal analysis.  Some unimportant things are discarded and finally, the memory usage is reduced using a code I found online (should give credit here).  Here a useful function is also defined that is used to build the filter for the data for different variations.  This function is used with build_dataframe to get the datasets for analysis after cuts applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "current_location = 'macbook'\n",
    "file_compression = 'bz2'\n",
    "load_test_batch  = False\n",
    "\n",
    "file_path = {}\n",
    "file_path['imac'] = '/Users/dmriser/data/inclusive_kaon/inclusive_kaon_small.csv'\n",
    "file_path['macbook'] = '/Users/davidriser/Data/inclusive/inclusive_kaon_small.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6786077 events occupying 1604 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "if load_test_batch:\n",
    "    data = pd.read_csv(file_path[current_location],\n",
    "                      compression=file_compression, \n",
    "                      nrows = 100000)\n",
    "else:\n",
    "    data = pd.read_csv(file_path[current_location],\n",
    "                      compression=file_compression)\n",
    "\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'Unnamed: 0.1', u'helicity', u'meson_id',\n",
       "       u'missing_mass', u'x', u'q2', u'z', u'pt', u'w', u'eta', u'phi_h',\n",
       "       u'theta_h', u'p_ele', u'p_mes', u'phi_ele', u'phi_mes', u'theta_ele',\n",
       "       u'theta_mes', u'dvz', u'alpha', u'dist_ecsf', u'dist_ec_edep',\n",
       "       u'dist_vz', u'dist_cc_theta', u'dist_dcr1', u'dist_dcr3', u'dist_ecu',\n",
       "       u'dist_ecv', u'dist_ecw', u'dist_cc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_filter(data,conf=None):\n",
    "    '''\n",
    "    data: This is the dataframe, we only need\n",
    "    it to check that the variable is indeed there.\n",
    "    \n",
    "    conf: A dict that contains the \n",
    "    cut name and the min, max values\n",
    "    to be used.  Anything not in this dict\n",
    "    will be assigned the nominal value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # When the filter is too long, pandas.DataFrame.query() breaks.\n",
    "    # Use a vector of filters instead.\n",
    "    filters = []\n",
    "\n",
    "    # basic thing that always applies\n",
    "    filters.append('q2 > 1.0')\n",
    "    \n",
    "    if 'w' in data.columns:\n",
    "        filters.append('w > 2.0')\n",
    "\n",
    "    if 'meson_id' in data.columns:\n",
    "        filters.append('meson_id == 321')\n",
    "    \n",
    "    # nominal values \n",
    "    nominal_conf = {}    \n",
    "    nominal_conf['alpha']         = [0.05, 1.0]\n",
    "    nominal_conf['dist_cc']       = [-1.0, 1.0]\n",
    "    nominal_conf['dist_cc_theta'] = [-1.0, 1.0]\n",
    "    nominal_conf['dist_dcr1']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_dcr3']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecsf']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecu']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecv']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecw']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ec_edep']  = [-1.0, 1.0]\n",
    "    nominal_conf['dist_vz']       = [-1.0, 1.0]\n",
    "    nominal_conf['missing_mass']  = [1.25, 5.0]\n",
    "    nominal_conf['p_mes']         = [0.35, 5.0]\n",
    "    nominal_conf['dvz']           = [-2.5, 2.5]\n",
    "\n",
    "        \n",
    "    # start adding the special options \n",
    "    if conf:\n",
    "        for k,v in conf.iteritems():\n",
    "\n",
    "            # these have to be valid \n",
    "            if len(v) is not 2:\n",
    "                print('Improper limits for parameter %s' % v)\n",
    "                return filters\n",
    "        \n",
    "            if k in data.columns:\n",
    "                filters.append('%s > %f and %s < %f' % (k,v[0],k,v[1]))\n",
    "                print('OPTION: %s, LIMITS: [%f,%f]' % (k,v[0],v[1]))\n",
    "            else:\n",
    "                print('Problem adding filter for %s because it is not in the dataframe.columns' % k)\n",
    "\n",
    "        # now add the default options for those which were not specified\n",
    "        for k,v in nominal_conf.iteritems():\n",
    "            if k not in conf.keys():\n",
    "                if k in data.columns:\n",
    "                    filters.append('%s > %f and %s < %f ' % (k,v[0],k,v[1])) \n",
    "                else:\n",
    "                    print('Problem adding filter for %s because it is not in the dataframe.columns' % k)\n",
    "            else:\n",
    "                print('Not adding nominal cut for %s, it was in the special cuts.' % k)\n",
    "    \n",
    "    else:\n",
    "        for k,v in nominal_conf.iteritems():\n",
    "            if k in data.columns:\n",
    "                print('OPTION: %s, LIMITS: [%f,%f]' % (k,v[0],v[1]))\n",
    "                filters.append('%s > %f and %s < %f ' % (k,v[0],k,v[1])) \n",
    "            else:\n",
    "                print('Problem adding filter for %s because it is not in the dataframe.columns' % k)        \n",
    "    \n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_dataframe(data, filters):\n",
    "    CHUNK_SIZE = 4\n",
    "       \n",
    "    if len(filters) < CHUNK_SIZE:\n",
    "        return data.query(' and '.join(filters))\n",
    "    \n",
    "    else:\n",
    "        d = data.copy(deep=True)\n",
    "        \n",
    "        for i in range(0, len(filters), CHUNK_SIZE):\n",
    "            f = filters[i:i + CHUNK_SIZE]\n",
    "            d.query(' and '.join(f), inplace=True)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6759179 events occupying 1082 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "IMPORTANT_AXES = ['alpha', 'dist_cc', 'dist_cc_theta', \n",
    "                  'dist_dcr1', 'dist_dcr3', 'dist_ecsf',\n",
    "                  'dist_ec_edep', 'dist_ecu', 'dist_ecv', \n",
    "                  'dist_ecw','dist_vz', 'helicity',\n",
    "                  'missing_mass', 'p_mes', 'phi_h', \n",
    "                  'pt', 'q2', 'x', 'z', 'dvz']\n",
    "\n",
    "data.dropna(how='any', inplace=True)\n",
    "\n",
    "for col in data.columns:\n",
    "    if col not in IMPORTANT_AXES:\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "                       \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6759179 events occupying 547 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "data, _ = reduce_mem_usage(data)\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (or load) Binning \n",
    "Binning is created by making equal statistics in each bin, for each axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_by_quantile(data, axis=None, n_bins=None):\n",
    "\n",
    "    # find minimum and maximum \n",
    "    axis_range = np.min(data[axis]), np.max(data[axis])\n",
    "    \n",
    "    # step in quantile to do binning \n",
    "    quantile_step = 1.0/n_bins\n",
    "    \n",
    "    bins = []\n",
    "    for index in range(n_bins+1):\n",
    "        bins.append(data[axis].quantile(index*quantile_step))\n",
    "        \n",
    "    return np.array(bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTION: dist_dcr3, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_dcr1, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_vz, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dvz, LIMITS: [-2.500000,2.500000]\n",
      "OPTION: dist_cc_theta, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecw, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecsf, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: p_mes, LIMITS: [0.350000,5.000000]\n",
      "OPTION: alpha, LIMITS: [0.050000,1.000000]\n",
      "OPTION: missing_mass, LIMITS: [1.250000,5.000000]\n",
      "OPTION: dist_ecu, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_cc, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecv, LIMITS: [-1.000000,1.000000]\n",
      "Loaded 3309431 events occupying 268 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "nominal_filter = build_filter(data)\n",
    "nominal_data   = build_dataframe(data, nominal_filter)\n",
    "\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(nominal_data), np.sum(nominal_data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bins = {}\n",
    "\n",
    "for axis in AXES:\n",
    "    \n",
    "    if axis is not 'z':\n",
    "        bins[axis] = bin_by_quantile(nominal_data.query('z > %f and z < %f' % (Z_RANGE[0], Z_RANGE[1])), \n",
    "                                                        axis=axis, n_bins=N_BINS)    \n",
    "    else:\n",
    "        bins[axis] = bin_by_quantile(nominal_data, axis=axis, n_bins=N_BINS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Nominal Result\n",
    "This part of the code generates a dataframe which contains the measurement results for the \"best\" parameters.  I will also add a global index to the dataframe.  An important parameter is defined which restricts the range of z added to non-z axes.  It is called z_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_bin_limits_to_centers(limits):\n",
    "    centers = []\n",
    "    for i in range(len(limits)-1):\n",
    "            centers.append(limits[i] + 0.5*(limits[i+1]-limits[i]))\n",
    "\n",
    "    return np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_asymmetry_df(data, axis, n_bins,\n",
    "                     beam_pol=0.749,\n",
    "                     beam_pol_unc=0.03,\n",
    "                     n_phi_bins=12,\n",
    "                     custom_bin_limits=None):\n",
    "\n",
    "    # setup the binning for the phi axis\n",
    "    phi_bin_limits = np.linspace(-180, 180, n_phi_bins+1)\n",
    "\n",
    "    # covert bin limits to central positions for plotting\n",
    "    phi_bin_centers = convert_bin_limits_to_centers(phi_bin_limits)\n",
    "\n",
    "    if custom_bin_limits is not None:\n",
    "        axis_range = list([custom_bin_limits[0], custom_bin_limits[-1]])\n",
    "        bin_limits = custom_bin_limits\n",
    "        n_bins = len(custom_bin_limits)-1\n",
    "\n",
    "    else:\n",
    "        # calculate range of binned axis\n",
    "        axis_range = data[axis].quantile(0.001), data[axis].quantile(0.999)\n",
    "\n",
    "        # create bin limits for binning up the dataframe\n",
    "        bin_limits = np.linspace(axis_range[0], axis_range[1], n_bins+1)\n",
    "\n",
    "    results = []\n",
    "    for index in range(len(bin_limits)-1):\n",
    "\n",
    "        # setup a string used to query for data in this bin\n",
    "        # and the corresponding title for this bin\n",
    "        bin_query = ('%s > %f and %s < %f' % (axis, bin_limits[index], axis, bin_limits[index+1]))\n",
    "        bin_title = ('%s $\\in [%.2f, %.2f]$' % (axis, bin_limits[index], bin_limits[index+1]))\n",
    "\n",
    "        # query the data for this bin\n",
    "        data_subset = data.query(bin_query)\n",
    "\n",
    "        # get histograms for positive and negative helicity\n",
    "        pos_counts, _ = np.histogram(data_subset[data_subset.helicity > 0].phi_h, bins=phi_bin_limits)\n",
    "        neg_counts, _ = np.histogram(data_subset[data_subset.helicity < 0].phi_h, bins=phi_bin_limits)\n",
    "\n",
    "        # calculate the asymmetry and the error\n",
    "        diff = np.array(pos_counts-neg_counts, dtype=np.float32)\n",
    "        total = np.array(pos_counts+neg_counts, dtype=np.float32)\n",
    "        asymmetry = diff/total/beam_pol\n",
    "        error = np.sqrt((1-asymmetry**2)/total)\n",
    "        sys0 = beam_pol_unc*np.abs(asymmetry)\n",
    "        \n",
    "        result = {}\n",
    "        result['axis']       = [axis] * len(pos_counts)\n",
    "        result['axis_min']   = [bin_limits[index]] * len(pos_counts)\n",
    "        result['axis_max']   = [bin_limits[index+1]] * len(pos_counts)\n",
    "        result['axis_bin']   = [index] * len(pos_counts)\n",
    "        result['counts_pos'] = pos_counts\n",
    "        result['counts_neg'] = neg_counts\n",
    "        result['value']      = asymmetry\n",
    "        result['stat']       = error\n",
    "        result['sys_0']      = sys0\n",
    "        result['phi']        = phi_bin_centers\n",
    "        result['phi_bin']    = np.arange(len(phi_bin_centers))\n",
    "        results.append(pd.DataFrame(result))\n",
    "\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results(data, axes, bins):\n",
    "    df_store = []\n",
    "\n",
    "    for axis in axes:\n",
    "        if axis is not 'z':\n",
    "            df_store.append( get_asymmetry_df(data=data.query('z > %f and z < %f' % (Z_RANGE[0], Z_RANGE[1])), \n",
    "                                                axis=axis, \n",
    "                                              n_bins=len(bins[axis]),\n",
    "                                              custom_bin_limits=bins[axis],\n",
    "                                             n_phi_bins=12) \n",
    "                           )\n",
    "        else:\n",
    "            df_store.append( get_asymmetry_df(data=data, \n",
    "                                                axis=axis, \n",
    "                                              n_bins=len(bins[axis]),\n",
    "                                              custom_bin_limits=bins[axis],\n",
    "                                             n_phi_bins=12) \n",
    "                           )\n",
    "     \n",
    "    for df in df_store:\n",
    "        df['global_index'] = df.phi_bin + df.axis_bin * len(np.unique(df.phi_bin))\n",
    "\n",
    "    return pd.concat(df_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['nominal'] = get_results(nominal_data, AXES, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Free the memory! \n",
    "del nominal_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary Parameters\n",
    "Now I will vary some parameters and store those results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variations          = {}\n",
    "\n",
    "variations['alpha'] = {}\n",
    "#variations['alpha'][-2] = [ 0.02, 1.0]\n",
    "variations['alpha'][-1] = [0.035, 1.0]\n",
    "variations['alpha'][ 0] = [ 0.05, 1.0]\n",
    "variations['alpha'][ 1] = [ 0.07, 1.0]\n",
    "#variations['alpha'][ 2] = [ 0.09, 1.0]\n",
    "\n",
    "variations['dist_cc'] = {}\n",
    "#variations['dist_cc'][-2] = [-1.2, 1.2]\n",
    "variations['dist_cc'][-1] = [-1.1, 1.1]\n",
    "variations['dist_cc'][ 0] = [-1.0, 1.0]\n",
    "#variations['dist_cc'][ 1] = [-0.9, 0.9]\n",
    "#variations['dist_cc'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_dcr1'] = {}\n",
    "#variations['dist_dcr1'][-2] = [-1.2, 1.2]\n",
    "variations['dist_dcr1'][-1] = [-1.1, 1.0]\n",
    "variations['dist_dcr1'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_dcr1'][ 1] = [-0.9, 1.0]\n",
    "#variations['dist_dcr1'][ 2] = [-0.8, 0.5]\n",
    "\n",
    "variations['dist_dcr3'] = {}\n",
    "#variations['dist_dcr3'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_dcr3'][-1] = [ -1.1, 1.0]\n",
    "variations['dist_dcr3'][ 0] = [ -1.0, 1.0]\n",
    "variations['dist_dcr3'][ 1] = [-0.95, 1.0]\n",
    "#variations['dist_dcr3'][ 2] = [-0.90, 0.45]\n",
    "\n",
    "variations['dist_ecsf'] = {}\n",
    "#variations['dist_ecsf'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ecsf'][-1] = [-1.1, 1.1]\n",
    "variations['dist_ecsf'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ecsf'][ 1] = [-0.9, 0.9]\n",
    "#variations['dist_ecsf'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ec_edep'] = {}\n",
    "#variations['dist_ec_edep'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ec_edep'][-1] = [-1.1, 1.0]\n",
    "variations['dist_ec_edep'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ec_edep'][ 1] = [-0.9, 1.0]\n",
    "#variations['dist_ec_edep'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ecu'] = {}\n",
    "#variations['dist_ecu'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ecu'][-1] = [-1.1, 1.1]\n",
    "variations['dist_ecu'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ecu'][ 1] = [-0.9, 0.9]\n",
    "#variations['dist_ecu'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ecv'] = {}\n",
    "#variations['dist_ecv'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_ecv'][-1] = [-1.0, 1.1]\n",
    "variations['dist_ecv'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ecv'][ 1] = [-1.0, 0.98]\n",
    "#variations['dist_ecv'][ 2] = [-0.45, 0.96]\n",
    "\n",
    "variations['dist_ecw'] = {}\n",
    "#variations['dist_ecw'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_ecw'][-1] = [ -1.0, 1.1]\n",
    "variations['dist_ecw'][ 0] = [ -1.0, 1.0]\n",
    "variations['dist_ecw'][ 1] = [ -1.0, 0.98]\n",
    "#variations['dist_ecw'][ 2] = [-0.55, 0.96]\n",
    "\n",
    "variations['dist_vz'] = {}\n",
    "#variations['dist_vz'][-2] = [-1.2, 1.2]\n",
    "variations['dist_vz'][-1] = [-1.1, 1.1]\n",
    "variations['dist_vz'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_vz'][ 1] = [-0.9, 0.9]\n",
    "#variations['dist_vz'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "'''\n",
    "The following variation is added only after using the \n",
    "results of the function called momentum_check.  These\n",
    "values are used to add the difference between the two \n",
    "as a systematic uncertainty.\n",
    "'''\n",
    "variations['p_mes'] = {}\n",
    "variations['p_mes'][-1] = [0.0, 2.5]\n",
    "variations['p_mes'][ 0] = [0.0, 5.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in variations.keys():\n",
    "    results[par] = {}\n",
    "    \n",
    "    for index in variations[par].keys():\n",
    "        print('\\nDoing  %.3f < %s %.3f' % (variations[par][index][0], par, \n",
    "                                         variations[par][index][1]))\n",
    "\n",
    "        # get these cut values \n",
    "        temp_dict = {}\n",
    "        temp_dict[par] = variations[par][index]\n",
    "        \n",
    "        # get data \n",
    "        temp_filter = build_filter(data, temp_dict)\n",
    "        temp_data   = build_dataframe(data, temp_filter)\n",
    "        results[par][index] = get_results(temp_data, AXES, bins)\n",
    "        del temp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_database(results, \n",
    "                     db_path='database/', \n",
    "                     naming_scheme='variation_%s_%s.csv'):\n",
    "    \n",
    "    for parameter in results.keys():\n",
    "        if parameter is not 'nominal':\n",
    "            for level in results[parameter].keys():\n",
    "                output_path = db_path + naming_scheme % (parameter, level)\n",
    "                results[parameter][level].to_csv(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_database(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Checks\n",
    "The following section is used to check the stability of the analysis.  First I will split the dataset into sub-samples and compute the nominal result on all sub-samples.  The results will be compared by using a chi-2 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metric(y1, y2, dy1, dy2):\n",
    "    return 0.5*np.average((y2-y1)**2/(0.5*(dy1+dy2)**2))\n",
    "    #return 1-np.sum((y2-y1)**2)/np.sum((y2+y1)**2)\n",
    "    \n",
    "def chi2(y_true, y_pred, y_err):\n",
    "    return np.average((y_true-y_pred)**2/y_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def subset_check(data, axes, bins, n_subsets=2):\n",
    "    \n",
    "    subsets = np.array_split(data, n_subsets)\n",
    "    \n",
    "    results = []\n",
    "    for subset in subsets:\n",
    "        results.append(get_results(subset, axes, bins))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(n_subsets):\n",
    "        for j in range(i+1,n_subsets):            \n",
    "            if i is not j:\n",
    "                score = metric(results[i].value, results[j].value, \n",
    "                               results[i].stat, results[j].stat)\n",
    "                scores.append(score)\n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nominal_filter = build_filter(data)\n",
    "nominal_data   = build_dataframe(data, nominal_filter)\n",
    "subset_scores  = subset_check(data, AXES, bins, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import binom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def randomize_helicity(data):\n",
    "    states = [-1, 1]\n",
    "    return np.random.choice(states, size=len(data))\n",
    "\n",
    "def random_helicity_check(data, axes, bins, n_trials):\n",
    "    scores  = []\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        data['helicity'] = randomize_helicity(data)\n",
    "        result = get_results(data, axes, bins)\n",
    "\n",
    "        # add the probability of this result or less \n",
    "        # to the dataframe\n",
    "        result['p_value'] = 1-np.abs(binom.cdf(result.counts_pos, \n",
    "                                  result.counts_neg+result.counts_pos, \n",
    "                                  0.5) - binom.cdf(result.counts_neg, \n",
    "                                  result.counts_neg+result.counts_pos, \n",
    "                                  0.5) \n",
    "                               )\n",
    "            \n",
    "        y_true = np.zeros(len(result))\n",
    "        scores.append(chi2(result.value, y_true, result.stat))\n",
    "        results.append(result)\n",
    "        \n",
    "    return scores, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hel_scores, hel_results = random_helicity_check(nominal_data, AXES, bins, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def momentum_check(data, axes, bins, p_limits):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for index in range(len(p_limits)):\n",
    "        query_string = 'p_mes > %f and p_mes < %f' % (p_limits[index][0], p_limits[index][1])\n",
    "        results.append(get_results(data.query(query_string), axes, bins))\n",
    "        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_limits = np.array([[0.35, 5.0], [0.35, 2.5]])\n",
    "p_results = momentum_check(data, AXES, bins, p_limits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del nominal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Systematic Errors\n",
    "Using the variation of parameters, the difference between each configuration and nominal will be calculated.  A column is then added to the nominal results for each source of systematic error which contains the largest shift from nominal for each global bin (this is done with the largest_shift routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for conf in results.keys():\n",
    "\n",
    "    if conf is not 'nominal':\n",
    "        for val in results[conf].keys():\n",
    "            \n",
    "            # Is this safe?  They could be in different orders.  It has been checked visually. \n",
    "            results[conf][val]['shift'] = results['nominal']['value'] - results[conf][val]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_global_bin_data(nominal, variations, axis, global_index):\n",
    "              \n",
    "    d = nominal.query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    v = {}\n",
    "    for var in variations.keys():\n",
    "        v[var] = variations[var].query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    found_data = True\n",
    "    \n",
    "    if len(d) is not 1:\n",
    "        found_data = False\n",
    "        \n",
    "    for vi in v.keys():\n",
    "        if len(v[vi]) is not 1:\n",
    "            found_data = False\n",
    "    \n",
    "    if not found_data:\n",
    "        print('Trouble finding data for global index %d' % global_index)\n",
    "        return \n",
    "\n",
    "    # success\n",
    "    return d,v\n",
    "\n",
    "def get_largest_shifts(results):\n",
    "    \n",
    "    '''\n",
    "    inputs \n",
    "    ------\n",
    "    results - A dictionary generated above which contains results of \n",
    "    nominal running, as well as parameter variations.  \n",
    "    \n",
    "    outputs \n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # we need to do the process for each axis independently\n",
    "    active_axes = np.unique(results['nominal'].axis)\n",
    "    n_global    = len(np.unique(results['nominal'].global_index))\n",
    "    \n",
    "    # something to store the result in \n",
    "    df_dict = {}\n",
    "    df_dict['axis']         = []\n",
    "    df_dict['global_index'] = []\n",
    "    \n",
    "    # somewhere to correlate the variation name with the column name\n",
    "    column_dict = {}\n",
    "    \n",
    "    # going though the different variations \n",
    "    i_par = 1\n",
    "    for par in results.keys(): \n",
    "        if par is not 'nominal':\n",
    "            # setup somewhere to store this \n",
    "            column_title = 'sys_%d' % i_par\n",
    "            df_dict[column_title] = []    \n",
    "            column_dict[column_title] = par\n",
    "            i_par += 1\n",
    "            \n",
    "    for axis in active_axes:\n",
    "        for index in range(n_global):\n",
    "            df_dict['axis'].append(axis)\n",
    "            df_dict['global_index'].append(index)\n",
    "            \n",
    "            i_par = 1\n",
    "            for par in results.keys(): \n",
    "                if par is not 'nominal':\n",
    "                    # setup somewhere to store this \n",
    "                    column_title = 'sys_%d' % i_par\n",
    "                \n",
    "                    d,v = get_global_bin_data(results['nominal'], results[par], axis, index)        \n",
    "                    current_shifts = [val['shift'].values[0] for key,val in v.iteritems()]\n",
    "                    df_dict[column_title].append(np.max(np.abs(current_shifts)))\n",
    "                    i_par += 1\n",
    "    \n",
    "    # now add them in quadrature\n",
    "    df_dict['sys_total'] = []\n",
    "    for i in range(len(df_dict['global_index'])):\n",
    "        \n",
    "        bin_total = 0.0\n",
    "        for k in df_dict.keys():\n",
    "            if 'sys' in k and 'total' not in k:\n",
    "                bin_total += df_dict[k][i]**2\n",
    "        \n",
    "        df_dict['sys_total'].append(bin_total)\n",
    "    \n",
    "    df_dict['sys_total'] = np.sqrt(df_dict['sys_total'])\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df, column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_linearized_error(results):\n",
    "    \n",
    "    '''\n",
    "    inputs \n",
    "    ------\n",
    "    results - A dictionary generated above which contains results of \n",
    "    nominal running, as well as parameter variations.  \n",
    "    \n",
    "    outputs \n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # we need to do the process for each axis independently\n",
    "    active_axes = np.unique(results['nominal'].axis)\n",
    "    n_global    = len(np.unique(results['nominal'].global_index))\n",
    "    \n",
    "    # something to store the result in \n",
    "    df_dict = {}\n",
    "    df_dict['axis']         = []\n",
    "    df_dict['global_index'] = []\n",
    "    \n",
    "    # somewhere to correlate the variation name with the column name\n",
    "    column_dict = {}\n",
    "    \n",
    "    # going though the different variations \n",
    "    i_par = 1\n",
    "    for par in results.keys(): \n",
    "        if par is not 'nominal':\n",
    "            # setup somewhere to store this \n",
    "            column_title = 'sys_%d' % i_par\n",
    "            df_dict[column_title] = []    \n",
    "            column_dict[column_title] = par\n",
    "            i_par += 1\n",
    "            \n",
    "    for axis in active_axes:\n",
    "        for index in range(n_global):\n",
    "            df_dict['axis'].append(axis)\n",
    "            df_dict['global_index'].append(index)\n",
    "            \n",
    "            i_par = 1\n",
    "            for par in results.keys(): \n",
    "                if par is not 'nominal':\n",
    "                    # setup somewhere to store this \n",
    "                    column_title = 'sys_%d' % i_par\n",
    "                \n",
    "                    d,v = get_global_bin_data(results['nominal'], results[par], axis, index)        \n",
    "\n",
    "                    if 1 in v.keys() and -1 in v.keys():\n",
    "                        delta = (v[1]['value'].values[0]-v[-1]['value'].values[0])\n",
    "                    elif 0 in v.keys() and -1 in v.keys():\n",
    "                        delta = (v[0]['value'].values[0]-v[-1]['value'].values[0])\n",
    "                    elif 0 in v.keys() and 1 in v.keys():\n",
    "                        delta = (v[1]['value'].values[0]-v[0]['value'].values[0])\n",
    "                    else:\n",
    "                        raise ValueError('For parameter %s I dont know how to linearize, there are no shifts?' % par)\n",
    "                        \n",
    "                    df_dict[column_title].append(delta)\n",
    "                    i_par += 1\n",
    "    \n",
    "    # now add them in quadrature\n",
    "    df_dict['sys_total'] = []\n",
    "    for i in range(len(df_dict['global_index'])):\n",
    "        \n",
    "        bin_total = 0.0\n",
    "        for k in df_dict.keys():\n",
    "            if 'sys' in k and 'total' not in k:\n",
    "                bin_total += df_dict[k][i]**2\n",
    "        \n",
    "        df_dict['sys_total'].append(bin_total)\n",
    "    \n",
    "    df_dict['sys_total'] = np.sqrt(df_dict['sys_total'])\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df, column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#shift_df, var_to_col = get_largest_shifts(results)\n",
    "shift_df, var_to_col = get_linearized_error(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_to_col['sys_0'] = 'beam_pol'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is very important.  I am adding the shifts into the nominal dataframe, but you must be careful to merge it using the axis and the global_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results['nominal'] = pd.merge(left=results['nominal'], \n",
    "                              right=shift_df, \n",
    "                              on=['axis', 'global_index']\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nominal'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_table(results, naming_dict):\n",
    "    \n",
    "    sys_cols = [c for c in results.columns if 'sys' in c and 'total' not in c]\n",
    "    \n",
    "    for col in sys_cols:\n",
    "        print('mean: %f, stat: %f, parameter: %s' % (np.average(np.abs(results[col])), \n",
    "                                                     np.average(results.stat), naming_dict[col])\n",
    "             )\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print_table(results['nominal'], var_to_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "The results are visualized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_results(results, axis, plot_sys=False):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "    phi_width = np.repeat(float(360.0/len(np.unique(d.phi_bin))), len(np.unique(d.phi_bin)))\n",
    "    phi_edges = np.linspace(-180,180,len(np.unique(d.phi_bin)))\n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.errorbar(x=dsub.phi, y=dsub.value, yerr=dsub.stat, \n",
    "                    linestyle='', marker='o', color='black')\n",
    "        \n",
    "        if 'sys_total' in dsub.columns and plot_sys is True:\n",
    "            plt.bar(phi_edges, height=dsub.sys_total, width=phi_width, bottom=0.0, \n",
    "                    edgecolor='black', color='red', alpha=0.65, hatch='///', \n",
    "                    label='systematic error', align='edge')\n",
    "            \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([-0.08, 0.08])\n",
    "        \n",
    "        plt.xlabel('$\\phi_h$')\n",
    "        plt.ylabel('BSA')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_sys(results, axis):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    sys_columns = [c for c in d.columns if 'sys' in c and 'total' not in c]\n",
    "    \n",
    "    # these are uniform width\n",
    "    phi_width = np.repeat(float(360.0/len(np.unique(d.phi_bin))), len(np.unique(d.phi_bin)))\n",
    "    phi_edges = np.linspace(-180,180,len(np.unique(d.phi_bin)))\n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        \n",
    "        total = np.zeros(len(np.unique(d.phi_bin)))\n",
    "        for sys_i in sys_columns:\n",
    "            plt.bar(phi_edges, height=dsub[sys_i], width=phi_width, bottom=total, label=sys_i, align='edge')\n",
    "            total += dsub[sys_i]\n",
    "            \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([0.0, 0.35])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_sys_bar(results, name_dict, axis):    \n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    \n",
    "    # get subset of data\n",
    "    d = results.query('axis == \"%s\"' % axis)\n",
    "    \n",
    "    # list of systematics \n",
    "    sys_columns = [c for c in d.columns if 'sys' in c and 'total' not in c]\n",
    "    names = [name_dict[c] for c in sys_columns]\n",
    "    \n",
    "    errs = [np.average(d[xp].values**2) for xp in sys_columns]\n",
    "    errs = np.array(errs)\n",
    "    errs /= np.sum(errs)\n",
    "    \n",
    "    n_sys = len(sys_columns)\n",
    "\n",
    "    plt.barh(np.arange(n_sys), errs, tick_label=names,\n",
    "            color='red', hatch='///', edgecolor='black', alpha=0.8)\n",
    "    \n",
    "    plt.xlabel('Fraction of total systematic uncertainty')\n",
    "    \n",
    "def compare_results(results1, results2, axis):\n",
    "    \n",
    "    d1     = results1.query('axis == \"%s\"' % axis)\n",
    "    d2     = results2.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d1.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "#    phi_width = float(360.0/len(np.unique(d.phi_bin)))\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 4*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub1 = d1.query('axis_bin == %d' % index)\n",
    "        dsub2 = d2.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.errorbar(x=dsub1.phi, y=dsub1.value, yerr=dsub1.stat, \n",
    "                    linestyle='', marker='o', color='red')\n",
    "        plt.errorbar(x=dsub2.phi, y=dsub2.value, yerr=dsub2.stat, \n",
    "                    linestyle='', marker='o', color='blue')\n",
    "       \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([-0.15, 0.15])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_counts(results, axis):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "    phi_width = float(360.0/len(np.unique(d.phi_bin)))\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 4*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.bar(dsub.phi, dsub.counts_pos, width=phi_width, \n",
    "                color='red', alpha=0.3)\n",
    "        plt.bar(dsub.phi, dsub.counts_neg, width=phi_width, \n",
    "                color='blue', alpha=0.3)        \n",
    "      \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def get_shifts(d, v):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    yerr = []\n",
    "    # key is the variation number\n",
    "    # value is the dataframe \n",
    "    for key, val in v.iteritems():\n",
    "        x.append(key)\n",
    "        y.append(val['value'].values[0])\n",
    "        yerr.append(np.sqrt(np.abs(val['stat'].values[0]**2 - d['stat'].values[0]**2)))\n",
    "     \n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y,       dtype=np.float32)\n",
    "    yerr = np.array(yerr, dtype=np.float32)\n",
    "    \n",
    "    return x, y, yerr\n",
    "\n",
    "def plot_shifts(nominal, variations, axis, global_index):\n",
    "    d,v = get_global_bin_data(nominal, variations, axis, global_index)\n",
    "    x, y, yerr = get_shifts(d, v)\n",
    "    \n",
    "    low = d.value.values[0]-d.stat.values[0]\n",
    "    up = d.value.values[0]+d.stat.values[0]\n",
    "    \n",
    "    # set color of bar \n",
    "    color = 'green'\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] > low) and (y[i] < up):\n",
    "            continue\n",
    "        else:\n",
    "            color='red'\n",
    "    \n",
    "    plt.errorbar(x=x, y=y, yerr=yerr, \n",
    "                 linestyle='', marker='o', color='black')\n",
    "    plt.axhspan(low, up, \n",
    "                color=color, alpha=0.2)\n",
    "   # plt.ylim([-0.06, 0.06])\n",
    "    plt.title('(phi,axis)=(%d,%d)' % (d.phi_bin, d.axis_bin))\n",
    "\n",
    "def plot_2d_shifts(nominal, variations, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi    = len(np.unique(nominal.phi_bin))\n",
    "    naxis   = len(np.unique(nominal.axis_bin))\n",
    "    nvars   = len(variations.keys())\n",
    "    \n",
    "    if nphi*naxis is not nglobal:\n",
    "        print('big problems')\n",
    "        return\n",
    "    \n",
    "    val = np.zeros([nvars, nglobal])\n",
    "    \n",
    "    for index in range(nglobal):\n",
    "        d,v = get_global_bin_data(nominal, variations, axis, index)\n",
    "        x, y, yerr = get_shifts(d, v)\n",
    "    \n",
    "        for i,yp in enumerate(y):\n",
    "            val[i][index] = yp\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(val)\n",
    "    \n",
    "def plot_grid_shifts(nominal, variations, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi = len(np.unique(nominal.phi_bin))\n",
    "    \n",
    "    #ncol = nphi\n",
    "    ncol = 6\n",
    "    nrow = 1+np.ceil(nglobal/ncol)\n",
    "    \n",
    "    plt.figure(figsize=(4*ncol,3*nrow))\n",
    "    for i in range(nglobal):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        plot_shifts(nominal, variations, axis, i)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_variation_limits(data, variations):\n",
    "    \n",
    "    n_vars = len(variations.keys())\n",
    "    n_col = 3\n",
    "    n_row = 1 + np.ceil(n_vars/n_col)\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    index = 1\n",
    "    for k,v in variations.iteritems():\n",
    "        plt.subplot(n_row, n_col, index)\n",
    "        plt.hist(data[k], histtype='stepfilled', bins=50,\n",
    "                alpha=0.65, edgecolor='black', color='red', hatch='///');\n",
    "        plt.xlabel(k)\n",
    "        index += 1\n",
    "\n",
    "        for key, values in variations[k].iteritems():\n",
    "            color = 'black'\n",
    "            \n",
    "            if key is 0:\n",
    "                color = 'green'\n",
    "            \n",
    "            plt.axvline(values[0], color=color)\n",
    "            plt.axvline(values[1], color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_subset_check(scores):\n",
    "    plt.hist(scores, bins=20, histtype='stepfilled',\n",
    "            color='red', edgecolor='black', hatch='///', alpha=0.7)\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Occurances')\n",
    "    plt.title('Subset Scores')\n",
    "    \n",
    "def plot_helicity_bin(nominal, results, axis, global_index):\n",
    "    bin_results = []\n",
    "    val = []\n",
    "    err = []\n",
    "    \n",
    "    nominal_data = nominal.query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    for r in results:\n",
    "        bin_results.append(r.query('axis == \"%s\" and global_index == %d' % (axis, global_index)))\n",
    "        \n",
    "    for b in bin_results:\n",
    "        val.append(b.value.values[0])\n",
    "        err.append(b.stat.values[0])\n",
    "        \n",
    "    val = np.array(val, dtype=np.float32)\n",
    "    err = np.array(err, dtype=np.float32)\n",
    "    \n",
    "    plt.hist(val, bins=np.linspace(-0.1, 0.1, 40), histtype='stepfilled',\n",
    "            color='yellow', edgecolor='red', hatch='///');\n",
    "    plt.axvspan(-1*nominal_data.stat.values[0], nominal_data.stat.values[0], alpha=0.4, color='red')\n",
    "    \n",
    "    \n",
    "def plot_grid_helicity_check(nominal, results, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi = len(np.unique(nominal.phi_bin))\n",
    "    \n",
    "    #ncol = nphi\n",
    "    ncol = 6\n",
    "    nrow = 1+np.ceil(nglobal/ncol)\n",
    "    \n",
    "    plt.figure(figsize=(4*ncol,3*nrow))\n",
    "    for i in range(nglobal):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        plot_helicity_bin(nominal, results, axis, i)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "    \n",
    "def plot_helicity_prob(results, axis):\n",
    "    \n",
    "    r = results.query('axis == \"%s\"' % axis)\n",
    "    \n",
    "    phi_width = np.repeat(float(360.0/len(np.unique(r.phi_bin))), len(np.unique(r.phi_bin)))\n",
    "    phi_edges = np.linspace(-180,180,len(np.unique(r.phi_bin)))\n",
    "\n",
    "    n_bins = len(np.unique(r.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = 2*(1+np.ceil(n_bins/n_col))\n",
    "    \n",
    "    plt.figure(figsize=(5*n_col, 2*n_row))\n",
    "\n",
    "    for i in range(n_bins):\n",
    "        pad = i if i < n_col else n_col + i\n",
    "        \n",
    "        plt.subplot(n_row, n_col, n_col+pad+1)\n",
    "        plt.bar(phi_edges, height=r.query('axis_bin == %d' % i).p_value, \n",
    "               width=phi_width, bottom=0.0, edgecolor='black', color='red', \n",
    "                alpha=0.65, hatch='///', label='CDF Value', align='edge')\n",
    "        plt.ylim([0.0,1.0])\n",
    "        plt.ylabel('p value')\n",
    "    \n",
    "    for i in range(n_bins):\n",
    "        pad = i if i < n_col else n_col + i\n",
    "\n",
    "        plt.subplot(n_row, n_col, pad+1)\n",
    "        plt.errorbar(r.query('axis_bin == %d' % i).phi, \n",
    "                     r.query('axis_bin == %d' % i).value, \n",
    "                     r.query('axis_bin == %d' % i).stat, color='black',marker='o',\n",
    "                    linestyle='')\n",
    "        plt.axhline(0.0, color='black', alpha=0.4, linestyle='--')\n",
    "        plt.ylim([-0.06,0.08])\n",
    "        plt.ylabel('BSA')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_kinematics(data):\n",
    "    plt.figure(figsize=(12,8))\n",
    "    \n",
    "    plt.subplot(2,2,1)\n",
    "    plt.hist2d(data.z, data.pt, bins=100, cmap='rainbow')\n",
    "    plt.colorbar()\n",
    "    plt.xlabel('z')\n",
    "    plt.ylabel('$P_T$')\n",
    "    \n",
    "    plt.subplot(2,2,2)\n",
    "    plt.hist(data.x, bins=100, histtype='stepfilled', hatch='///',\n",
    "            color='white', edgecolor='black') \n",
    "    plt.xlabel('x')\n",
    "    \n",
    "    plt.subplot(2,2,3)\n",
    "    plt.hist(data.z, bins=100, histtype='stepfilled', hatch='///',\n",
    "            color='white', edgecolor='black') \n",
    "    plt.xlabel('z')\n",
    "\n",
    "\n",
    "    plt.subplot(2,2,4)\n",
    "    plt.hist(data.q2, bins=100, histtype='stepfilled', hatch='///',\n",
    "            color='white', edgecolor='black')  \n",
    "    plt.xlabel('$Q^2$')\n",
    "\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_binning(data, bins):\n",
    "    \n",
    "    latex = {}\n",
    "    latex['x'] = 'x'\n",
    "    latex['z'] = 'z'\n",
    "    latex['q2'] = '$Q^2$'\n",
    "    latex['pt'] = '$P_T$'\n",
    "    \n",
    "    n_axes = len(bins.keys())\n",
    "    n_col = 2\n",
    "    n_row = 1+np.ceil(n_axes/n_col)\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 4*n_row))\n",
    "\n",
    "    for index, axis in enumerate(bins.keys()):\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.hist(data[axis], bins=100, histtype='stepfilled',\n",
    "                color='orange', edgecolor='black', alpha=0.5)\n",
    "    \n",
    "        plt.xlabel(latex[axis])\n",
    "        for b in bins[axis]:\n",
    "            plt.axvline(b, color='black')\n",
    "            \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_momentum_check(results, axis, p_limits):\n",
    "    \n",
    "    colors = ['red', 'blue', 'black', 'green']\n",
    "    \n",
    "    df_store = []\n",
    "    for r in results:\n",
    "        df_store.append(r.query('axis == \"%s\"' % axis))\n",
    "    \n",
    "    if len(df_store) is 2:\n",
    "        r = lambda x1, x2: 1 - np.sum((x2-x1)**2)/np.sum((x2+x1)**2)\n",
    "        m = r(df_store[0].value, df_store[1].value)\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    for index, df in enumerate(df_store):\n",
    "        \n",
    "        label_string = '$p \\in [%.2f, %.2f]$' % (p_limits[index][0], p_limits[index][1])\n",
    "        plt.errorbar(df.global_index, df.value, df.stat, \n",
    "                     linestyle='', marker='.', label=label_string,\n",
    "                    color=colors[index])\n",
    "    \n",
    "    plt.ylim([-0.06, 0.06])\n",
    "    plt.axhline(0.0, linestyle='--', linewidth=1, color='black', alpha=0.7)\n",
    "    plt.ylabel('BSA')\n",
    "    plt.xlabel('Global bin index')\n",
    "    plt.legend()\n",
    "    \n",
    "#    if len(df_store) is 2:\n",
    "#        plt.text(2, 0.05, '$R = %.2f$' % m)\n",
    "\n",
    "def scatter_two_results(result1, result2, axis):\n",
    "    \n",
    "    d1 = result1.query('axis == \"%s\"' % axis)\n",
    "    d2 = result2.query('axis == \"%s\"' % axis)\n",
    "    axis_val = d1.axis_min + 0.5*(d1.axis_max - d1.axis_min)\n",
    "    \n",
    "    plt.figure(figsize=(16,9))\n",
    "    plt.scatter(d1.value, d2.value, \n",
    "                marker='o', alpha=0.75, c=axis_val, cmap='viridis')\n",
    "    plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_binning(build_dataframe(data, nominal_filter),\n",
    "             bins)\n",
    "plt.savefig('image/binning.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_kinematics(build_dataframe(data, nominal_filter))\n",
    "plt.savefig('image/kinematics.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for axis in AXES:\n",
    "    \n",
    "    save_title = 'image/bsa_%s.pdf' % axis\n",
    "    plot_results(results['nominal'], axis)\n",
    "    plt.savefig(save_title, bbox_inches='tight')\n",
    "\n",
    "    save_title = 'image/bsa_%s_sys.pdf' % axis\n",
    "    plot_results(results['nominal'], axis, plot_sys=True)\n",
    "    plt.savefig(save_title, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for axis in AXES:\n",
    "    plot_sys_bar(results['nominal'], var_to_col, axis)\n",
    "    save_title = 'image/systematic-bar-%s.pdf' % axis\n",
    "    plt.savefig(save_title, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_sys(results['nominal'], 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"shift\" plots display the different results obtained by varying different analysis parameters (\"cuts\").  The horizontal band corresponds to the statistical error associated with the nominal value, with the \"ideal\" set of cuts.  The color of the band indicates the true/false status of whether a measurement has fallen outside of the statistical error band or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['alpha'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['p_mes'], 'z')\n",
    "#plt.savefig('image/variation-p_mes.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['dist_vz'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_shifts(results['nominal'], results['dist_ecu'], 'x', 47)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_2d_shifts(results['nominal'], results['dist_vz'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variation_limits(data, variations)\n",
    "#plt.savefig('image/variations.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_subset_check(subset_scores)\n",
    "#plt.savefig('image/subset_scores_k4.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_momentum_check(p_results, 'x', p_limits)\n",
    "plt.savefig('image/p-study-x.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#scatter_two_results(p_results[0], p_results[1], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_grid_helicity_check(results['nominal'], hel_results, 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_helicity_prob(hel_results[0], 'z')\n",
    "#plt.savefig('image/random-helicity.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = build_dataframe(data, nominal_filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist2d(df.x, df.q2, bins=400, norm=LogNorm(),\n",
    "          range=[[0.05, 0.65],[0.95, 5.0]]);\n",
    "plt.colorbar()\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$Q^2$')\n",
    "plt.savefig('image/x-q2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "plt.hist2d(df.z, df.pt**2, bins=400, norm=LogNorm(),\n",
    "          range=[[0.05, 1.0],[-0.05, 1.2]]);\n",
    "plt.colorbar()\n",
    "plt.xlabel('$z$')\n",
    "plt.ylabel('$P_T^2$')\n",
    "plt.savefig('image/z-pt2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results\n",
    "The results for this measurement, with systematic uncertainties, is saved in simple csv format for fitting in a different code (which as of this moment does not exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nominal'].to_csv('results/phi-dist-sys.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
