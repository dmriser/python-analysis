{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About \n",
    "This notebook is a new way to do the analysis.  Starting from the data and ending at the phi-dependent asymmetries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "plt.rc('font', size=18)\n",
    "plt.rc('font', family='serif')\n",
    "\n",
    "AXES = ['x', 'z', 'pt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "Load the data from a compressed csv file.  Dropping the events which are not used in the nominal analysis.  Some unimportant things are discarded and finally, the memory usage is reduced using a code I found online (should give credit here).  Here a useful function is also defined that is used to build the filter for the data for different variations.  This function is used with build_dataframe to get the datasets for analysis after cuts applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2708801 events occupying 619 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "#data = pd.read_csv('/Users/dmriser/data/inclusive_kaon/inclusive_kaon.csv', compression='bz2')\n",
    "#data = pd.read_csv('/Users/davidriser/Data/inclusive/inclusive_kaon.csv', compression='bz2')\n",
    "data = pd.read_csv('/home/david/data/inclusive/inclusive_kaon.csv', \n",
    "                   compression='bz2')\n",
    "\n",
    "\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'helicity', u'meson_id', u'missing_mass', u'x', u'q2',\n",
       "       u'z', u'pt', u'w', u'eta', u'phi_h', u'theta_h', u'p_ele', u'p_mes',\n",
       "       u'phi_ele', u'phi_mes', u'theta_ele', u'theta_mes', u'dvz', u'alpha',\n",
       "       u'dist_ecsf', u'dist_ec_edep', u'dist_vz', u'dist_cc_theta',\n",
       "       u'dist_dcr1', u'dist_dcr3', u'dist_ecu', u'dist_ecv', u'dist_ecw',\n",
       "       u'dist_cc'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_filter(data,conf=None):\n",
    "    '''\n",
    "    data: This is the dataframe, we only need\n",
    "    it to check that the variable is indeed there.\n",
    "    \n",
    "    conf: A dict that contains the \n",
    "    cut name and the min, max values\n",
    "    to be used.  Anything not in this dict\n",
    "    will be assigned the nominal value.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # When the filter is too long, pandas.DataFrame.query() breaks.\n",
    "    # Use a vector of filters instead.\n",
    "    filters = []\n",
    "\n",
    "    # basic thing that always applies\n",
    "    filters.append('q2 > 1.0')\n",
    "    \n",
    "    if 'w' in data.columns:\n",
    "        filters.append('w > 2.0')\n",
    "\n",
    "    if 'meson_id' in data.columns:\n",
    "        filters.append('meson_id == 321')\n",
    "    \n",
    "    # nominal values \n",
    "    nominal_conf = {}    \n",
    "    nominal_conf['alpha']         = [0.05, 1.0]\n",
    "    nominal_conf['dist_cc']       = [-1.0, 1.0]\n",
    "    nominal_conf['dist_cc_theta'] = [-1.0, 1.0]\n",
    "    nominal_conf['dist_dcr1']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_dcr3']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecsf']     = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecu']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecv']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ecw']      = [-1.0, 1.0]\n",
    "    nominal_conf['dist_ec_edep']  = [-1.0, 1.0]\n",
    "    nominal_conf['dist_vz']       = [-1.0, 1.0]\n",
    "    nominal_conf['missing_mass']  = [1.25, 5.0]\n",
    "    nominal_conf['p_mes']         = [ 0.0, 5.0]\n",
    "\n",
    "        \n",
    "    # start adding the special options \n",
    "    if conf:\n",
    "        for k,v in conf.iteritems():\n",
    "\n",
    "            # these have to be valid \n",
    "            if len(v) is not 2:\n",
    "                print('Improper limits for parameter %s' % v)\n",
    "                return filters\n",
    "        \n",
    "            if k in data.columns:\n",
    "                filters.append('%s > %f and %s < %f' % (k,v[0],k,v[1]))\n",
    "                print('OPTION: %s, LIMITS: [%f,%f]' % (k,v[0],v[1]))\n",
    "            else:\n",
    "                print('Problem adding filter for %s because it is not in the dataframe.columns' % k)\n",
    "\n",
    "        # now add the default options for those which were not specified\n",
    "        for k,v in nominal_conf.iteritems():\n",
    "            if k not in conf.keys():\n",
    "                if k in data.columns:\n",
    "                    filters.append('%s > %f and %s < %f ' % (k,v[0],k,v[1])) \n",
    "                else:\n",
    "                    print('Problem adding filter for %s because it is not in the dataframe.columns' % k)\n",
    "            else:\n",
    "                print('Not adding nominal cut for %s, it was in the special cuts.' % k)\n",
    "    \n",
    "    else:\n",
    "        for k,v in nominal_conf.iteritems():\n",
    "            if k in data.columns:\n",
    "                print('OPTION: %s, LIMITS: [%f,%f]' % (k,v[0],v[1]))\n",
    "                filters.append('%s > %f and %s < %f ' % (k,v[0],k,v[1])) \n",
    "            else:\n",
    "                print('Problem adding filter for %s because it is not in the dataframe.columns' % k)        \n",
    "    \n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataframe(data, filters):\n",
    "    CHUNK_SIZE = 4\n",
    "       \n",
    "    if len(filters) < CHUNK_SIZE:\n",
    "        return data.query(' and '.join(filters))\n",
    "    \n",
    "    else:\n",
    "        d = data.copy(deep=True)\n",
    "        \n",
    "        for i in range(0, len(filters), CHUNK_SIZE):\n",
    "            f = filters[i:i + CHUNK_SIZE]\n",
    "            d.query(' and '.join(f), inplace=True)\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2674996 events occupying 408 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "IMPORTANT_AXES = ['alpha', 'dist_cc', 'dist_cc_theta', \n",
    "                  'dist_dcr1', 'dist_dcr3', 'dist_ecsf',\n",
    "                  'dist_ec_edep', 'dist_ecu', 'dist_ecv', \n",
    "                  'dist_ecw','dist_vz', 'helicity',\n",
    "                  'missing_mass', 'p_mes', 'phi_h', \n",
    "                  'pt', 'q2', 'x', 'z']\n",
    "\n",
    "data.dropna(how='any', inplace=True)\n",
    "\n",
    "for col in data.columns:\n",
    "    if col not in IMPORTANT_AXES:\n",
    "        data.drop(col, axis=1, inplace=True)\n",
    "        \n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object:  # Exclude strings\n",
    "                       \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "            \n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2674996 events occupying 206 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "data, _ = reduce_mem_usage(data)\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(data), np.sum(data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Binning \n",
    "Binning is created somewhere else, it's too complicated for this document! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "bins = {}\n",
    "\n",
    "for axis in AXES:\n",
    "    bins[axis] = pkl.load(open('bins/plus_' + axis + '.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Nominal Result\n",
    "This part of the code generates a dataframe which contains the measurement results for the \"best\" parameters.  I will also add a global index to the dataframe.  An important parameter is defined which restricts the range of z added to non-z axes.  It is called z_range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_RANGE = [0.25, 0.75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_bin_limits_to_centers(limits):\n",
    "    centers = []\n",
    "    for i in range(len(limits)-1):\n",
    "            centers.append(limits[i] + 0.5*(limits[i+1]-limits[i]))\n",
    "    return np.array(centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_asymmetry_df(data, axis, n_bins,\n",
    "                     beam_pol=0.749,\n",
    "                     beam_pol_unc=0.03,\n",
    "                     n_phi_bins=12,\n",
    "                     custom_bin_limits=None):\n",
    "\n",
    "    # setup the binning for the phi axis\n",
    "    phi_bin_limits = np.linspace(-180, 180, n_phi_bins+1)\n",
    "\n",
    "    # covert bin limits to central positions for plotting\n",
    "    phi_bin_centers = convert_bin_limits_to_centers(phi_bin_limits)\n",
    "\n",
    "    if custom_bin_limits is not None:\n",
    "        axis_range = list([custom_bin_limits[0], custom_bin_limits[-1]])\n",
    "        bin_limits = custom_bin_limits\n",
    "        n_bins = len(custom_bin_limits)-1\n",
    "\n",
    "    else:\n",
    "        # calculate range of binned axis\n",
    "        axis_range = data[axis].quantile(0.001), data[axis].quantile(0.999)\n",
    "\n",
    "        # create bin limits for binning up the dataframe\n",
    "        bin_limits = np.linspace(axis_range[0], axis_range[1], n_bins+1)\n",
    "\n",
    "    results = []\n",
    "    for index in range(len(bin_limits)-1):\n",
    "\n",
    "        # setup a string used to query for data in this bin\n",
    "        # and the corresponding title for this bin\n",
    "        bin_query = ('%s > %f and %s < %f' % (axis, bin_limits[index], axis, bin_limits[index+1]))\n",
    "        bin_title = ('%s $\\in [%.2f, %.2f]$' % (axis, bin_limits[index], bin_limits[index+1]))\n",
    "\n",
    "        # query the data for this bin\n",
    "        data_subset = data.query(bin_query)\n",
    "\n",
    "        # get histograms for positive and negative helicity\n",
    "        pos_counts, _ = np.histogram(data_subset[data_subset.helicity > 0].phi_h, bins=phi_bin_limits)\n",
    "        neg_counts, _ = np.histogram(data_subset[data_subset.helicity < 0].phi_h, bins=phi_bin_limits)\n",
    "\n",
    "        # calculate the asymmetry and the error\n",
    "        diff = np.array(pos_counts-neg_counts, dtype=np.float32)\n",
    "        total = np.array(pos_counts+neg_counts, dtype=np.float32)\n",
    "        asymmetry = diff/total/beam_pol\n",
    "        error = np.sqrt((1-asymmetry**2)/total)\n",
    "        sys0 = beam_pol_unc*np.abs(asymmetry)\n",
    "        \n",
    "        result = {}\n",
    "        result['axis']       = [axis] * len(pos_counts)\n",
    "        result['axis_min']   = [bin_limits[index]] * len(pos_counts)\n",
    "        result['axis_max']   = [bin_limits[index+1]] * len(pos_counts)\n",
    "        result['axis_bin']   = [index] * len(pos_counts)\n",
    "        result['counts_pos'] = pos_counts\n",
    "        result['counts_neg'] = neg_counts\n",
    "        result['value']      = asymmetry\n",
    "        result['stat']       = error\n",
    "        result['sys_0']      = sys0\n",
    "        result['phi']        = phi_bin_centers\n",
    "        result['phi_bin']    = np.arange(len(phi_bin_centers))\n",
    "        results.append(pd.DataFrame(result))\n",
    "\n",
    "    return pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTION: dist_dcr3, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_dcr1, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_vz, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_cc_theta, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecw, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecsf, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: p_mes, LIMITS: [0.000000,5.000000]\n",
      "OPTION: alpha, LIMITS: [0.050000,1.000000]\n",
      "OPTION: missing_mass, LIMITS: [1.250000,5.000000]\n",
      "OPTION: dist_ecu, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_cc, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecv, LIMITS: [-1.000000,1.000000]\n",
      "Loaded 1232100 events occupying 95 MB of memory.\n"
     ]
    }
   ],
   "source": [
    "nominal_filter = build_filter(data)\n",
    "nominal_data   = build_dataframe(data, nominal_filter)\n",
    "print('Loaded %d events occupying %d MB of memory.' % (len(nominal_data), np.sum(nominal_data.memory_usage()/1024**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(data, axes, bins):\n",
    "    df_store = []\n",
    "\n",
    "    for axis in axes:\n",
    "        if axis is not 'z':\n",
    "            df_store.append( get_asymmetry_df(data=data.query('z > %f and z < %f' % (Z_RANGE[0], Z_RANGE[1])), \n",
    "                                                axis=axis, \n",
    "                                              n_bins=len(bins[axis]),\n",
    "                                              custom_bin_limits=bins[axis],\n",
    "                                             n_phi_bins=12) \n",
    "                           )\n",
    "        else:\n",
    "            df_store.append( get_asymmetry_df(data=data, \n",
    "                                                axis=axis, \n",
    "                                              n_bins=len(bins[axis]),\n",
    "                                              custom_bin_limits=bins[axis],\n",
    "                                             n_phi_bins=12) \n",
    "                           )\n",
    "     \n",
    "    for df in df_store:\n",
    "        df['global_index'] = df.phi_bin + df.axis_bin * len(np.unique(df.phi_bin))\n",
    "\n",
    "    return pd.concat(df_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "results['nominal'] = get_results(nominal_data, AXES, bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free the memory! \n",
    "del nominal_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vary Parameters\n",
    "Now I will vary some parameters and store those results as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "variations          = {}\n",
    "\n",
    "variations['alpha'] = {}\n",
    "variations['alpha'][-2] = [ 0.02, 1.0]\n",
    "variations['alpha'][-1] = [0.035, 1.0]\n",
    "variations['alpha'][ 0] = [ 0.05, 1.0]\n",
    "variations['alpha'][ 1] = [ 0.07, 1.0]\n",
    "variations['alpha'][ 2] = [ 0.09, 1.0]\n",
    "\n",
    "variations['dist_cc'] = {}\n",
    "variations['dist_cc'][-2] = [-1.2, 1.2]\n",
    "variations['dist_cc'][-1] = [-1.1, 1.1]\n",
    "variations['dist_cc'][ 0] = [-1.0, 1.0]\n",
    "#variations['dist_cc'][ 1] = [-0.9, 0.9]\n",
    "#variations['dist_cc'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_dcr1'] = {}\n",
    "variations['dist_dcr1'][-2] = [-1.2, 1.2]\n",
    "variations['dist_dcr1'][-1] = [-1.1, 1.1]\n",
    "variations['dist_dcr1'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_dcr1'][ 1] = [-0.9, 0.6]\n",
    "variations['dist_dcr1'][ 2] = [-0.8, 0.5]\n",
    "\n",
    "variations['dist_dcr3'] = {}\n",
    "variations['dist_dcr3'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_dcr3'][-1] = [ -1.1, 1.1]\n",
    "variations['dist_dcr3'][ 0] = [ -1.0, 1.0]\n",
    "variations['dist_dcr3'][ 1] = [-0.95, 0.65]\n",
    "variations['dist_dcr3'][ 2] = [-0.90, 0.45]\n",
    "\n",
    "variations['dist_ecsf'] = {}\n",
    "variations['dist_ecsf'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ecsf'][-1] = [-1.1, 1.1]\n",
    "variations['dist_ecsf'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ecsf'][ 1] = [-0.9, 0.9]\n",
    "variations['dist_ecsf'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ec_edep'] = {}\n",
    "variations['dist_ec_edep'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ec_edep'][-1] = [-1.1, 1.1]\n",
    "variations['dist_ec_edep'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ec_edep'][ 1] = [-0.9, 0.9]\n",
    "variations['dist_ec_edep'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ecu'] = {}\n",
    "variations['dist_ecu'][-2] = [-1.2, 1.2]\n",
    "variations['dist_ecu'][-1] = [-1.1, 1.1]\n",
    "variations['dist_ecu'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_ecu'][ 1] = [-0.9, 0.9]\n",
    "variations['dist_ecu'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['dist_ecv'] = {}\n",
    "variations['dist_ecv'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_ecv'][-1] = [ -1.1, 1.1]\n",
    "variations['dist_ecv'][ 0] = [ -1.0, 1.0]\n",
    "variations['dist_ecv'][ 1] = [-0.55, 0.9]\n",
    "variations['dist_ecv'][ 2] = [-0.45, 0.8]\n",
    "\n",
    "variations['dist_ecw'] = {}\n",
    "variations['dist_ecw'][-2] = [ -1.2, 1.2]\n",
    "variations['dist_ecw'][-1] = [ -1.1, 1.1]\n",
    "variations['dist_ecw'][ 0] = [ -1.0, 1.0]\n",
    "variations['dist_ecw'][ 1] = [ -0.6, 0.9]\n",
    "variations['dist_ecw'][ 2] = [-0.55, 0.8]\n",
    "\n",
    "variations['dist_vz'] = {}\n",
    "variations['dist_vz'][-2] = [-1.2, 1.2]\n",
    "variations['dist_vz'][-1] = [-1.1, 1.1]\n",
    "variations['dist_vz'][ 0] = [-1.0, 1.0]\n",
    "variations['dist_vz'][ 1] = [-0.9, 0.9]\n",
    "variations['dist_vz'][ 2] = [-0.8, 0.8]\n",
    "\n",
    "variations['p_mes'] = {}\n",
    "variations['p_mes'][-3] = [0.0, 2.5]\n",
    "variations['p_mes'][-2] = [0.0, 3.0]\n",
    "variations['p_mes'][-1] = [0.0, 3.5]\n",
    "variations['p_mes'][ 0] = [0.0, 5.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Doing  -1.000 < dist_dcr3 1.000\n",
      "OPTION: dist_dcr3, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_dcr3, it was in the special cuts.\n",
      "\n",
      "Doing  -0.950 < dist_dcr3 0.650\n",
      "OPTION: dist_dcr3, LIMITS: [-0.950000,0.650000]\n",
      "Not adding nominal cut for dist_dcr3, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_dcr3 0.450\n",
      "OPTION: dist_dcr3, LIMITS: [-0.900000,0.450000]\n",
      "Not adding nominal cut for dist_dcr3, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_dcr3 1.100\n",
      "OPTION: dist_dcr3, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_dcr3, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_dcr3 1.200\n",
      "OPTION: dist_dcr3, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_dcr3, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_dcr1 1.000\n",
      "OPTION: dist_dcr1, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_dcr1, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_dcr1 0.600\n",
      "OPTION: dist_dcr1, LIMITS: [-0.900000,0.600000]\n",
      "Not adding nominal cut for dist_dcr1, it was in the special cuts.\n",
      "\n",
      "Doing  -0.800 < dist_dcr1 0.500\n",
      "OPTION: dist_dcr1, LIMITS: [-0.800000,0.500000]\n",
      "Not adding nominal cut for dist_dcr1, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_dcr1 1.100\n",
      "OPTION: dist_dcr1, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_dcr1, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_dcr1 1.200\n",
      "OPTION: dist_dcr1, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_dcr1, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_vz 1.000\n",
      "OPTION: dist_vz, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_vz, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_vz 0.900\n",
      "OPTION: dist_vz, LIMITS: [-0.900000,0.900000]\n",
      "Not adding nominal cut for dist_vz, it was in the special cuts.\n",
      "\n",
      "Doing  -0.800 < dist_vz 0.800\n",
      "OPTION: dist_vz, LIMITS: [-0.800000,0.800000]\n",
      "Not adding nominal cut for dist_vz, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_vz 1.100\n",
      "OPTION: dist_vz, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_vz, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_vz 1.200\n",
      "OPTION: dist_vz, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_vz, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_ecw 1.000\n",
      "OPTION: dist_ecw, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_ecw, it was in the special cuts.\n",
      "\n",
      "Doing  -0.600 < dist_ecw 0.900\n",
      "OPTION: dist_ecw, LIMITS: [-0.600000,0.900000]\n",
      "Not adding nominal cut for dist_ecw, it was in the special cuts.\n",
      "\n",
      "Doing  -0.550 < dist_ecw 0.800\n",
      "OPTION: dist_ecw, LIMITS: [-0.550000,0.800000]\n",
      "Not adding nominal cut for dist_ecw, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_ecw 1.100\n",
      "OPTION: dist_ecw, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_ecw, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_ecw 1.200\n",
      "OPTION: dist_ecw, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_ecw, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_ec_edep 1.000\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_ec_edep, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_ec_edep 0.900\n",
      "OPTION: dist_ec_edep, LIMITS: [-0.900000,0.900000]\n",
      "Not adding nominal cut for dist_ec_edep, it was in the special cuts.\n",
      "\n",
      "Doing  -0.800 < dist_ec_edep 0.800\n",
      "OPTION: dist_ec_edep, LIMITS: [-0.800000,0.800000]\n",
      "Not adding nominal cut for dist_ec_edep, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_ec_edep 1.100\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_ec_edep, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_ec_edep 1.200\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_ec_edep, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_ecsf 1.000\n",
      "OPTION: dist_ecsf, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_ecsf, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_ecsf 0.900\n",
      "OPTION: dist_ecsf, LIMITS: [-0.900000,0.900000]\n",
      "Not adding nominal cut for dist_ecsf, it was in the special cuts.\n",
      "\n",
      "Doing  -0.800 < dist_ecsf 0.800\n",
      "OPTION: dist_ecsf, LIMITS: [-0.800000,0.800000]\n",
      "Not adding nominal cut for dist_ecsf, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_ecsf 1.100\n",
      "OPTION: dist_ecsf, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_ecsf, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_ecsf 1.200\n",
      "OPTION: dist_ecsf, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_ecsf, it was in the special cuts.\n",
      "\n",
      "Doing  0.000 < p_mes 5.500\n",
      "OPTION: p_mes, LIMITS: [0.000000,5.500000]\n",
      "Not adding nominal cut for p_mes, it was in the special cuts.\n",
      "\n",
      "Doing  0.000 < p_mes 3.500\n",
      "OPTION: p_mes, LIMITS: [0.000000,3.500000]\n",
      "Not adding nominal cut for p_mes, it was in the special cuts.\n",
      "\n",
      "Doing  0.000 < p_mes 2.500\n",
      "OPTION: p_mes, LIMITS: [0.000000,2.500000]\n",
      "Not adding nominal cut for p_mes, it was in the special cuts.\n",
      "\n",
      "Doing  0.000 < p_mes 3.000\n",
      "OPTION: p_mes, LIMITS: [0.000000,3.000000]\n",
      "Not adding nominal cut for p_mes, it was in the special cuts.\n",
      "\n",
      "Doing  0.050 < alpha 1.000\n",
      "OPTION: alpha, LIMITS: [0.050000,1.000000]\n",
      "Not adding nominal cut for alpha, it was in the special cuts.\n",
      "\n",
      "Doing  0.070 < alpha 1.000\n",
      "OPTION: alpha, LIMITS: [0.070000,1.000000]\n",
      "Not adding nominal cut for alpha, it was in the special cuts.\n",
      "\n",
      "Doing  0.090 < alpha 1.000\n",
      "OPTION: alpha, LIMITS: [0.090000,1.000000]\n",
      "Not adding nominal cut for alpha, it was in the special cuts.\n",
      "\n",
      "Doing  0.035 < alpha 1.000\n",
      "OPTION: alpha, LIMITS: [0.035000,1.000000]\n",
      "Not adding nominal cut for alpha, it was in the special cuts.\n",
      "\n",
      "Doing  0.020 < alpha 1.000\n",
      "OPTION: alpha, LIMITS: [0.020000,1.000000]\n",
      "Not adding nominal cut for alpha, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_ecu 1.000\n",
      "OPTION: dist_ecu, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_ecu, it was in the special cuts.\n",
      "\n",
      "Doing  -0.900 < dist_ecu 0.900\n",
      "OPTION: dist_ecu, LIMITS: [-0.900000,0.900000]\n",
      "Not adding nominal cut for dist_ecu, it was in the special cuts.\n",
      "\n",
      "Doing  -0.800 < dist_ecu 0.800\n",
      "OPTION: dist_ecu, LIMITS: [-0.800000,0.800000]\n",
      "Not adding nominal cut for dist_ecu, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_ecu 1.100\n",
      "OPTION: dist_ecu, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_ecu, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_ecu 1.200\n",
      "OPTION: dist_ecu, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_ecu, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_cc 1.000\n",
      "OPTION: dist_cc, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_cc, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_cc 1.100\n",
      "OPTION: dist_cc, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_cc, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_cc 1.200\n",
      "OPTION: dist_cc, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_cc, it was in the special cuts.\n",
      "\n",
      "Doing  -1.000 < dist_ecv 1.000\n",
      "OPTION: dist_ecv, LIMITS: [-1.000000,1.000000]\n",
      "Not adding nominal cut for dist_ecv, it was in the special cuts.\n",
      "\n",
      "Doing  -0.550 < dist_ecv 0.900\n",
      "OPTION: dist_ecv, LIMITS: [-0.550000,0.900000]\n",
      "Not adding nominal cut for dist_ecv, it was in the special cuts.\n",
      "\n",
      "Doing  -0.450 < dist_ecv 0.800\n",
      "OPTION: dist_ecv, LIMITS: [-0.450000,0.800000]\n",
      "Not adding nominal cut for dist_ecv, it was in the special cuts.\n",
      "\n",
      "Doing  -1.100 < dist_ecv 1.100\n",
      "OPTION: dist_ecv, LIMITS: [-1.100000,1.100000]\n",
      "Not adding nominal cut for dist_ecv, it was in the special cuts.\n",
      "\n",
      "Doing  -1.200 < dist_ecv 1.200\n",
      "OPTION: dist_ecv, LIMITS: [-1.200000,1.200000]\n",
      "Not adding nominal cut for dist_ecv, it was in the special cuts.\n"
     ]
    }
   ],
   "source": [
    "for par in variations.keys():\n",
    "    results[par] = {}\n",
    "    \n",
    "    for index in variations[par].keys():\n",
    "        print('\\nDoing  %.3f < %s %.3f' % (variations[par][index][0], par, \n",
    "                                         variations[par][index][1]))\n",
    "\n",
    "        # get these cut values \n",
    "        temp_dict = {}\n",
    "        temp_dict[par] = variations[par][index]\n",
    "        \n",
    "        # get data \n",
    "        temp_filter = build_filter(data, temp_dict)\n",
    "        temp_data   = build_dataframe(data, temp_filter)\n",
    "        results[par][index] = get_results(temp_data, AXES, bins)\n",
    "        del temp_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stability Checks\n",
    "The following section is used to check the stability of the analysis.  First I will split the dataset into sub-samples and compute the nominal result on all sub-samples.  The results will be compared by using a chi-2 test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(y1, y2, dy1, dy2):\n",
    "    return 0.5*np.average((y2-y1)**2/(0.5*(dy1+dy2)**2))\n",
    "    #return 1-np.sum((y2-y1)**2)/np.sum((y2+y1)**2)\n",
    "    \n",
    "def chi2(y_true, y_pred, y_err):\n",
    "    return np.average((y_true-y_pred)**2/y_err**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_check(data, axes, bins, n_subsets=2):\n",
    "    \n",
    "    subsets = np.array_split(data, n_subsets)\n",
    "    \n",
    "    results = []\n",
    "    for subset in subsets:\n",
    "        results.append(get_results(subset, axes, bins))\n",
    "    \n",
    "    scores = []\n",
    "    for i in range(n_subsets):\n",
    "        for j in range(i+1,n_subsets):            \n",
    "            if i is not j:\n",
    "                score = metric(results[i].value, results[j].value, \n",
    "                               results[i].stat, results[j].stat)\n",
    "                scores.append(score)\n",
    "                \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPTION: dist_dcr3, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_dcr1, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_vz, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_cc_theta, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecw, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ec_edep, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecsf, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: p_mes, LIMITS: [0.000000,5.000000]\n",
      "OPTION: alpha, LIMITS: [0.050000,1.000000]\n",
      "OPTION: missing_mass, LIMITS: [1.250000,5.000000]\n",
      "OPTION: dist_ecu, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_cc, LIMITS: [-1.000000,1.000000]\n",
      "OPTION: dist_ecv, LIMITS: [-1.000000,1.000000]\n"
     ]
    }
   ],
   "source": [
    "nominal_filter = build_filter(data)\n",
    "nominal_data   = build_dataframe(data, nominal_filter)\n",
    "subset_scores = subset_check(data, AXES, bins, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_helicity(data):\n",
    "    states = [-1, 1]\n",
    "    return np.random.choice(states, size=len(data))\n",
    "\n",
    "def random_helicity_check(data, axes, bins, n_trials):\n",
    "    scores  = []\n",
    "    results = []\n",
    "\n",
    "    for i in range(n_trials):\n",
    "        data['helicity'] = randomize_helicity(data)\n",
    "        result = get_results(data, axes, bins)\n",
    "        y_true = np.zeros(len(result))\n",
    "        scores.append(chi2(result.value, y_true, result.stat))\n",
    "        results.append(result)\n",
    "        \n",
    "    return scores, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "hel_scores, hel_results = random_helicity_check(nominal_data, AXES, bins, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "del nominal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Systematic Errors\n",
    "Using the variation of parameters, the difference between each configuration and nominal will be calculated.  A column is then added to the nominal results for each source of systematic error which contains the largest shift from nominal for each global bin (this is done with the largest_shift routine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for conf in results.keys():\n",
    "\n",
    "    if conf is not 'nominal':\n",
    "        for val in results[conf].keys():\n",
    "            \n",
    "            # Is this safe?  They could be in different orders.  It has been checked visually. \n",
    "            results[conf][val]['shift'] = results['nominal']['value'] - results[conf][val]['value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_global_bin_data(nominal, variations, axis, global_index):\n",
    "              \n",
    "    d = nominal.query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    v = {}\n",
    "    for var in variations.keys():\n",
    "        v[var] = variations[var].query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    found_data = True\n",
    "    \n",
    "    if len(d) is not 1:\n",
    "        found_data = False\n",
    "        \n",
    "    for vi in v.keys():\n",
    "        if len(v[vi]) is not 1:\n",
    "            found_data = False\n",
    "    \n",
    "    if not found_data:\n",
    "        print('Trouble finding data for global index %d' % global_index)\n",
    "        return \n",
    "\n",
    "    # success\n",
    "    return d,v\n",
    "\n",
    "def get_largest_shifts(results):\n",
    "    \n",
    "    '''\n",
    "    inputs \n",
    "    ------\n",
    "    results - A dictionary generated above which contains results of \n",
    "    nominal running, as well as parameter variations.  \n",
    "    \n",
    "    outputs \n",
    "    -------\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # we need to do the process for each axis independently\n",
    "    active_axes = np.unique(results['nominal'].axis)\n",
    "    n_global    = len(np.unique(results['nominal'].global_index))\n",
    "    \n",
    "    # something to store the result in \n",
    "    df_dict = {}\n",
    "    df_dict['axis']         = []\n",
    "    df_dict['global_index'] = []\n",
    "    \n",
    "    # somewhere to correlate the variation name with the column name\n",
    "    column_dict = {}\n",
    "    \n",
    "    # going though the different variations \n",
    "    i_par = 1\n",
    "    for par in results.keys(): \n",
    "        if par is not 'nominal':\n",
    "            # setup somewhere to store this \n",
    "            column_title = 'sys_%d' % i_par\n",
    "            df_dict[column_title] = []    \n",
    "            column_dict[column_title] = par\n",
    "            i_par += 1\n",
    "            \n",
    "    for axis in active_axes:\n",
    "        for index in range(n_global):\n",
    "            df_dict['axis'].append(axis)\n",
    "            df_dict['global_index'].append(index)\n",
    "            \n",
    "            i_par = 1\n",
    "            for par in results.keys(): \n",
    "                if par is not 'nominal':\n",
    "                    # setup somewhere to store this \n",
    "                    column_title = 'sys_%d' % i_par\n",
    "                \n",
    "                    d,v = get_global_bin_data(results['nominal'], results[par], axis, index)        \n",
    "                    current_shifts = [val['shift'].values[0] for key,val in v.iteritems()]\n",
    "                    df_dict[column_title].append(np.max(np.abs(current_shifts)))\n",
    "                    i_par += 1\n",
    "    \n",
    "    # now add them in quadrature\n",
    "    df_dict['sys_total'] = []\n",
    "    for i in range(len(df_dict['global_index'])):\n",
    "        \n",
    "        bin_total = 0.0\n",
    "        for k in df_dict.keys():\n",
    "            if 'sys' in k and 'total' not in k:\n",
    "                bin_total += df_dict[k][i]**2\n",
    "        \n",
    "        df_dict['sys_total'].append(bin_total)\n",
    "        \n",
    "    df_dict['sys_total'] = np.sqrt(df_dict['sys_total'])\n",
    "    df = pd.DataFrame(df_dict)\n",
    "    return df, column_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_df, var_to_col = get_largest_shifts(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_to_col['sys_0'] = 'beam_pol'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line is very important.  I am adding the shifts into the nominal dataframe, but you must be careful to merge it using the axis and the global_index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nominal'] = pd.merge(left=results['nominal'], \n",
    "                              right=shift_df, \n",
    "                              on=['axis', 'global_index']\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>axis</th>\n",
       "      <th>axis_bin</th>\n",
       "      <th>axis_max</th>\n",
       "      <th>axis_min</th>\n",
       "      <th>counts_neg</th>\n",
       "      <th>counts_pos</th>\n",
       "      <th>phi</th>\n",
       "      <th>phi_bin</th>\n",
       "      <th>stat</th>\n",
       "      <th>sys_0</th>\n",
       "      <th>...</th>\n",
       "      <th>sys_11</th>\n",
       "      <th>sys_2</th>\n",
       "      <th>sys_3</th>\n",
       "      <th>sys_4</th>\n",
       "      <th>sys_5</th>\n",
       "      <th>sys_6</th>\n",
       "      <th>sys_7</th>\n",
       "      <th>sys_8</th>\n",
       "      <th>sys_9</th>\n",
       "      <th>sys_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>16436</td>\n",
       "      <td>16311</td>\n",
       "      <td>-165.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.005526</td>\n",
       "      <td>0.000153</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012388</td>\n",
       "      <td>0.000382</td>\n",
       "      <td>0.006687</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.001769</td>\n",
       "      <td>0.004440</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.016184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>5994</td>\n",
       "      <td>5694</td>\n",
       "      <td>-135.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.009244</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036935</td>\n",
       "      <td>0.000559</td>\n",
       "      <td>0.007469</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.006159</td>\n",
       "      <td>0.004268</td>\n",
       "      <td>0.007956</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.008234</td>\n",
       "      <td>0.046834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>12239</td>\n",
       "      <td>11837</td>\n",
       "      <td>-105.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.006443</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009165</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.009500</td>\n",
       "      <td>0.010151</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.008598</td>\n",
       "      <td>0.003775</td>\n",
       "      <td>0.002740</td>\n",
       "      <td>0.002257</td>\n",
       "      <td>0.025650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>5800</td>\n",
       "      <td>5776</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.009294</td>\n",
       "      <td>0.000083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007065</td>\n",
       "      <td>0.000468</td>\n",
       "      <td>0.008975</td>\n",
       "      <td>0.007215</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.006447</td>\n",
       "      <td>0.010675</td>\n",
       "      <td>0.004319</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.024239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>0</td>\n",
       "      <td>0.150192</td>\n",
       "      <td>0.106729</td>\n",
       "      <td>4752</td>\n",
       "      <td>4481</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010399</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>...</td>\n",
       "      <td>0.014624</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.011015</td>\n",
       "      <td>0.005194</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.005907</td>\n",
       "      <td>0.010280</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.008271</td>\n",
       "      <td>0.024337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  axis  axis_bin  axis_max  axis_min  counts_neg  counts_pos    phi  phi_bin  \\\n",
       "0    x         0  0.150192  0.106729       16436       16311 -165.0        0   \n",
       "1    x         0  0.150192  0.106729        5994        5694 -135.0        1   \n",
       "2    x         0  0.150192  0.106729       12239       11837 -105.0        2   \n",
       "3    x         0  0.150192  0.106729        5800        5776  -75.0        3   \n",
       "4    x         0  0.150192  0.106729        4752        4481  -45.0        4   \n",
       "\n",
       "       stat     sys_0    ...        sys_11     sys_2     sys_3     sys_4  \\\n",
       "0  0.005526  0.000153    ...      0.012388  0.000382  0.006687  0.003003   \n",
       "1  0.009244  0.001028    ...      0.036935  0.000559  0.007469  0.002108   \n",
       "2  0.006443  0.000669    ...      0.009165  0.000639  0.009500  0.010151   \n",
       "3  0.009294  0.000083    ...      0.007065  0.000468  0.008975  0.007215   \n",
       "4  0.010399  0.001176    ...      0.014624  0.000750  0.011015  0.005194   \n",
       "\n",
       "      sys_5     sys_6     sys_7     sys_8     sys_9  sys_total  \n",
       "0  0.000823  0.001769  0.004440  0.001737  0.001579   0.016184  \n",
       "1  0.006159  0.004268  0.007956  0.001319  0.008234   0.046834  \n",
       "2  0.000039  0.008598  0.003775  0.002740  0.002257   0.025650  \n",
       "3  0.001219  0.006447  0.010675  0.004319  0.003017   0.024239  \n",
       "4  0.002070  0.005907  0.010280  0.001377  0.008271   0.024337  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results['nominal'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_table(results, naming_dict):\n",
    "    \n",
    "    sys_cols = [c for c in results.columns if 'sys' in c and 'total' not in c]\n",
    "    \n",
    "    for col in sys_cols:\n",
    "        print('mean: %f, stat: %f, parameter: %s' % (np.average(results[col]), \n",
    "                                                     np.average(results.stat), naming_dict[col])\n",
    "             )\n",
    "             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean: 0.000676, stat: 0.009835, parameter: beam_pol\n",
      "mean: 0.004614, stat: 0.009835, parameter: dist_dcr3\n",
      "mean: 0.009305, stat: 0.009835, parameter: dist_ecw\n",
      "mean: 0.008504, stat: 0.009835, parameter: dist_ecv\n",
      "mean: 0.003498, stat: 0.009835, parameter: dist_dcr1\n",
      "mean: 0.005372, stat: 0.009835, parameter: dist_ecu\n",
      "mean: 0.008172, stat: 0.009835, parameter: alpha\n",
      "mean: 0.001643, stat: 0.009835, parameter: dist_cc\n",
      "mean: 0.006468, stat: 0.009835, parameter: dist_ec_edep\n",
      "mean: 0.006704, stat: 0.009835, parameter: p_mes\n",
      "mean: 0.002468, stat: 0.009835, parameter: dist_ecsf\n",
      "mean: 0.005601, stat: 0.009835, parameter: dist_vz\n"
     ]
    }
   ],
   "source": [
    "print_table(results['nominal'], var_to_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "The results are visualized.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(results, axis):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "    phi_width = np.repeat(float(360.0/len(np.unique(d.phi_bin))), len(np.unique(d.phi_bin)))\n",
    "    phi_edges = np.linspace(-180,180,len(np.unique(d.phi_bin)))\n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.errorbar(x=dsub.phi, y=dsub.value, yerr=dsub.stat, \n",
    "                    linestyle='', marker='o', color='black')\n",
    "        \n",
    "        if 'sys_total' in dsub.columns:\n",
    "            plt.bar(phi_edges, height=dsub.sys_total, width=phi_width, bottom=0.0, \n",
    "                    edgecolor='red', color='yellow', alpha=0.65, hatch='///', \n",
    "                    label='systematic error', align='edge')\n",
    "            \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([-0.15, 0.15])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "\n",
    "def plot_sys(results, axis):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    sys_columns = [c for c in d.columns if 'sys' in c and 'total' not in c]\n",
    "    \n",
    "    # these are uniform width\n",
    "    phi_width = np.repeat(float(360.0/len(np.unique(d.phi_bin))), len(np.unique(d.phi_bin)))\n",
    "    phi_edges = np.linspace(-180,180,len(np.unique(d.phi_bin)))\n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        \n",
    "        total = np.zeros(len(np.unique(d.phi_bin)))\n",
    "        for sys_i in sys_columns:\n",
    "            plt.bar(phi_edges, height=dsub[sys_i], width=phi_width, bottom=total, label=sys_i, align='edge')\n",
    "            total += dsub[sys_i]\n",
    "            \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([0.0, 0.35])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def compare_results(results1, results2, axis):\n",
    "    \n",
    "    d1     = results1.query('axis == \"%s\"' % axis)\n",
    "    d2     = results2.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d1.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "#    phi_width = float(360.0/len(np.unique(d.phi_bin)))\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 4*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub1 = d1.query('axis_bin == %d' % index)\n",
    "        dsub2 = d2.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.errorbar(x=dsub1.phi, y=dsub1.value, yerr=dsub1.stat, \n",
    "                    linestyle='', marker='o', color='red')\n",
    "        plt.errorbar(x=dsub2.phi, y=dsub2.value, yerr=dsub2.stat, \n",
    "                    linestyle='', marker='o', color='blue')\n",
    "       \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        plt.ylim([-0.15, 0.15])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_counts(results, axis):\n",
    "    \n",
    "    d      = results.query('axis == \"%s\"' % axis)\n",
    "    n_bins = len(np.unique(d.axis_bin))\n",
    "    n_col  = 3\n",
    "    n_row  = np.ceil(n_bins/n_col)+1\n",
    "\n",
    "    # these are uniform width\n",
    "    phi_width = float(360.0/len(np.unique(d.phi_bin)))\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 4*n_row))\n",
    "    \n",
    "    for index in range(n_bins):\n",
    "        dsub = d.query('axis_bin == %d' % index)\n",
    "        plt.subplot(n_row, n_col, index+1)\n",
    "        plt.bar(dsub.phi, dsub.counts_pos, width=phi_width, \n",
    "                color='red', alpha=0.3)\n",
    "        plt.bar(dsub.phi, dsub.counts_neg, width=phi_width, \n",
    "                color='blue', alpha=0.3)        \n",
    "      \n",
    "        plt.axhline(0.0, linestyle='--',\n",
    "                    linewidth=1, color='black', alpha=0.4)\n",
    "        \n",
    "        plt.xlim([-180,180])\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def get_shifts(d, v):\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    yerr = []\n",
    "    # key is the variation number\n",
    "    # value is the dataframe \n",
    "    for key, val in v.iteritems():\n",
    "        x.append(key)\n",
    "        y.append(val['value'].values[0])\n",
    "        yerr.append(np.sqrt(np.abs(val['stat'].values[0]**2 - d['stat'].values[0]**2)))\n",
    "     \n",
    "    \n",
    "    x = np.array(x)\n",
    "    y = np.array(y,       dtype=np.float32)\n",
    "    yerr = np.array(yerr, dtype=np.float32)\n",
    "    \n",
    "    return x, y, yerr\n",
    "\n",
    "def plot_shifts(nominal, variations, axis, global_index):\n",
    "    d,v = get_global_bin_data(nominal, variations, axis, global_index)\n",
    "    x, y, yerr = get_shifts(d, v)\n",
    "    \n",
    "    low = d.value.values[0]-d.stat.values[0]\n",
    "    up = d.value.values[0]+d.stat.values[0]\n",
    "    \n",
    "    # set color of bar \n",
    "    color = 'green'\n",
    "    for i in range(len(y)):\n",
    "        if (y[i] > low) and (y[i] < up):\n",
    "            continue\n",
    "        else:\n",
    "            color='red'\n",
    "    \n",
    "    plt.errorbar(x=x, y=y, yerr=yerr, \n",
    "                 linestyle='', marker='o', color='black')\n",
    "    plt.axhspan(low, up, \n",
    "                color=color, alpha=0.2)\n",
    "    plt.ylim([-0.06, 0.06])\n",
    "    plt.title('(phi,axis)=(%d,%d)' % (d.phi_bin, d.axis_bin))\n",
    "\n",
    "def plot_2d_shifts(nominal, variations, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi    = len(np.unique(nominal.phi_bin))\n",
    "    naxis   = len(np.unique(nominal.axis_bin))\n",
    "    nvars   = len(variations.keys())\n",
    "    \n",
    "    if nphi*naxis is not nglobal:\n",
    "        print('big problems')\n",
    "        return\n",
    "    \n",
    "    val = np.zeros([nvars, nglobal])\n",
    "    \n",
    "    for index in range(nglobal):\n",
    "        d,v = get_global_bin_data(nominal, variations, axis, index)\n",
    "        x, y, yerr = get_shifts(d, v)\n",
    "    \n",
    "        for i,yp in enumerate(y):\n",
    "            val[i][index] = yp\n",
    "    \n",
    "    plt.figure(figsize=(16,16))\n",
    "    plt.imshow(val)\n",
    "    \n",
    "def plot_grid_shifts(nominal, variations, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi = len(np.unique(nominal.phi_bin))\n",
    "    \n",
    "    #ncol = nphi\n",
    "    ncol = 6\n",
    "    nrow = 1+np.ceil(nglobal/ncol)\n",
    "    \n",
    "    plt.figure(figsize=(4*ncol,3*nrow))\n",
    "    for i in range(nglobal):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        plot_shifts(nominal, variations, axis, i)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_variation_limits(data, variations):\n",
    "    \n",
    "    n_vars = len(variations.keys())\n",
    "    n_col = 3\n",
    "    n_row = 1 + np.ceil(n_vars/n_col)\n",
    "    \n",
    "    plt.figure(figsize=(4*n_col, 3*n_row))\n",
    "    \n",
    "    index = 1\n",
    "    for k,v in variations.iteritems():\n",
    "        plt.subplot(n_row, n_col, index)\n",
    "        plt.hist(data[k], histtype='stepfilled', bins=50,\n",
    "                alpha=0.65, edgecolor='black', color='red', hatch='///');\n",
    "        plt.xlabel(k)\n",
    "        index += 1\n",
    "\n",
    "        for key, values in variations[k].iteritems():\n",
    "            color = 'black'\n",
    "            \n",
    "            if key is 0:\n",
    "                color = 'green'\n",
    "            \n",
    "            plt.axvline(values[0], color=color)\n",
    "            plt.axvline(values[1], color=color)\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    \n",
    "def plot_subset_check(scores):\n",
    "    plt.hist(scores, bins=20, histtype='stepfilled',\n",
    "            color='red', edgecolor='black', hatch='///', alpha=0.7)\n",
    "    plt.xlabel('Metric')\n",
    "    plt.ylabel('Occurances')\n",
    "    plt.title('Subset Scores')\n",
    "    \n",
    "def plot_helicity_bin(nominal, results, axis, global_index):\n",
    "    bin_results = []\n",
    "    val = []\n",
    "    err = []\n",
    "    \n",
    "    nominal_data = nominal.query('axis == \"%s\" and global_index == %d' % (axis, global_index))\n",
    "    \n",
    "    for r in results:\n",
    "        bin_results.append(r.query('axis == \"%s\" and global_index == %d' % (axis, global_index)))\n",
    "        \n",
    "    for b in bin_results:\n",
    "        val.append(b.value.values[0])\n",
    "        err.append(b.stat.values[0])\n",
    "        \n",
    "    val = np.array(val, dtype=np.float32)\n",
    "    err = np.array(err, dtype=np.float32)\n",
    "    \n",
    "    plt.hist(val, bins=np.linspace(-0.1, 0.1, 40), histtype='stepfilled',\n",
    "            color='yellow', edgecolor='red', hatch='///');\n",
    "    plt.axvspan(-1*nominal_data.stat.values[0], nominal_data.stat.values[0], alpha=0.4, color='red')\n",
    "    \n",
    "    \n",
    "def plot_grid_helicity_check(nominal, results, axis):\n",
    "    \n",
    "    nglobal = len(np.unique(nominal.global_index))\n",
    "    nphi = len(np.unique(nominal.phi_bin))\n",
    "    \n",
    "    #ncol = nphi\n",
    "    ncol = 6\n",
    "    nrow = 1+np.ceil(nglobal/ncol)\n",
    "    \n",
    "    plt.figure(figsize=(4*ncol,3*nrow))\n",
    "    for i in range(nglobal):\n",
    "        plt.subplot(nrow,ncol,i+1)\n",
    "        plot_helicity_bin(nominal, results, axis, i)\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_results(results['nominal'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot_sys(results['nominal'], 'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These \"shift\" plots display the different results obtained by varying different analysis parameters (\"cuts\").  The horizontal band corresponds to the statistical error associated with the nominal value, with the \"ideal\" set of cuts.  The color of the band indicates the true/false status of whether a measurement has fallen outside of the statistical error band or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['alpha'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['p_mes'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grid_shifts(results['nominal'], results['dist_vz'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_2d_shifts(results['nominal'], results['dist_vz'], 'z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_variation_limits(data, variations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_subset_check(subset_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_grid_helicity_check(results['nominal'], hel_results, 'z')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Results\n",
    "The results for this measurement, with systematic uncertainties, is saved in simple csv format for fitting in a different code (which as of this moment does not exist)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['nominal'].to_csv('results/phi-dist.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
